# -*- coding: utf-8 -*-
"""app.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PVSSWcxh56vQJ_89tgWxZMjku36C_REm
"""

import gradio as gr
import torch
import torch.nn as nn
from torchvision import transforms
from PIL import Image
import pickle
import numpy as np

# model classes
class Encoder(nn.Module):
    def __init__(self, input_size=2048, hidden_size=512):
        super(Encoder, self).__init__()
        # projection layer
        self.fc = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.5)

    def forward(self, features):
        # projecting
        out = self.fc(features)
        out = self.relu(out)
        out = self.dropout(out)
        return out

class Decoder(nn.Module):
    def __init__(self, vocab_size, embed_size=256, hidden_size=512, num_layers=2):
        super(Decoder, self).__init__()
        # decoder layers
        self.embed = nn.Embedding(vocab_size, embed_size)
        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, vocab_size)
        self.dropout = nn.Dropout(0.5)
        self.hidden_size = hidden_size
        self.num_layers = num_layers

    def forward(self, features, captions):
        # getting embeddings
        embeddings = self.embed(captions)
        embeddings = self.dropout(embeddings)

        # hidden state stuff
        features = features.unsqueeze(0).repeat(self.num_layers, 1, 1)
        cell = torch.zeros_like(features)

        # lstm processing
        outputs, _ = self.lstm(embeddings, (features, cell))
        outputs = self.fc(outputs)
        return outputs

class ImageCaptioningModel(nn.Module):
    def __init__(self, vocab_size, embed_size=256, hidden_size=512):
        super(ImageCaptioningModel, self).__init__()
        # model parts
        self.encoder = Encoder(2048, hidden_size)
        self.decoder = Decoder(vocab_size, embed_size, hidden_size)

    def forward(self, features, captions):
        # encoding and decoding
        encoded = self.encoder(features)
        outputs = self.decoder(encoded, captions)
        return outputs

# vocabulary class
class Vocabulary:
    def __init__(self):
        # tokens
        self.word2idx = {}
        self.idx2word = {}

    def __len__(self):
        return len(self.word2idx)

# loading stuff
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# loading vocab
with open('vocab.pkl', 'rb') as f:
    vocab = pickle.load(f)

# loading model
model = ImageCaptioningModel(len(vocab)).to(device)
model.load_state_dict(torch.load('caption_model.pth', map_location=device))
model.eval()

# loading resnet for features
from torchvision import models
resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)
resnet = nn.Sequential(*list(resnet.children())[:-1])
resnet = resnet.to(device)
resnet.eval()

# transform
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))
])

# caption generation
def generate_caption(image, search_type="Greedy"):
    # processing image
    img = Image.fromarray(image).convert('RGB')
    img_tensor = transform(img).unsqueeze(0).to(device)

    # extracting features
    with torch.no_grad():
        feature = resnet(img_tensor).view(1, -1)
        feature = feature.cpu().numpy()[0]

    # generating caption
    if search_type == "Greedy":
        caption = greedy_search(model, feature, vocab, device=device)
    else:
        caption = beam_search(model, feature, vocab, device=device)

    return caption

# greedy search
def greedy_search(model, feature, vocab, max_len=50, device='cuda'):
    # setting up
    model.eval()
    result = []

    # generating
    with torch.no_grad():
        feature = torch.FloatTensor(feature).unsqueeze(0).to(device)
        encoded = model.encoder(feature)

        # starting token
        input_token = torch.LongTensor([[vocab.word2idx[vocab.start_token]]]).to(device)

        # hidden states
        hidden = encoded.unsqueeze(0).repeat(model.decoder.num_layers, 1, 1)
        cell = torch.zeros_like(hidden)

        # generating words
        for _ in range(max_len):
            embedding = model.decoder.embed(input_token)
            output, (hidden, cell) = model.decoder.lstm(embedding, (hidden, cell))
            output = model.decoder.fc(output.squeeze(1))

            # next token
            predicted = output.argmax(1)
            predicted_idx = predicted.item()

            # checking end
            if predicted_idx == vocab.word2idx[vocab.end_token]:
                break

            result.append(vocab.idx2word[predicted_idx])
            input_token = predicted.unsqueeze(0)

    return ' '.join(result)

# beam search
def beam_search(model, feature, vocab, beam_width=3, max_len=50, device='cuda'):
    # setting up
    model.eval()

    # encoding
    with torch.no_grad():
        feature = torch.FloatTensor(feature).unsqueeze(0).to(device)
        encoded = model.encoder(feature)

        # initial stuff
        hidden = encoded.unsqueeze(0).repeat(model.decoder.num_layers, 1, 1)
        cell = torch.zeros_like(hidden)

        # beams
        beams = [([vocab.word2idx[vocab.start_token]], 0.0, hidden, cell)]
        completed = []

        # searching
        for _ in range(max_len):
            candidates = []

            # expanding
            for sequence, score, h, c in beams:
                if sequence[-1] == vocab.word2idx[vocab.end_token]:
                    completed.append((sequence, score))
                    continue

                # predictions
                input_token = torch.LongTensor([[sequence[-1]]]).to(device)
                embedding = model.decoder.embed(input_token)
                output, (new_h, new_c) = model.decoder.lstm(embedding, (h, c))
                output = model.decoder.fc(output.squeeze(1))

                # top tokens
                probs = torch.log_softmax(output, dim=1)
                top_probs, top_indices = probs.topk(beam_width)

                # adding to candidates
                for prob, idx in zip(top_probs[0], top_indices[0]):
                    new_seq = sequence + [idx.item()]
                    new_score = score + prob.item()
                    candidates.append((new_seq, new_score, new_h, new_c))

            # selecting beams
            beams = sorted(candidates, key=lambda x: x[1], reverse=True)[:beam_width]

            if not beams:
                break

        # finalizing
        completed.extend(beams)

        if completed:
            best_seq = max(completed, key=lambda x: x[1])[0]
            result = [vocab.idx2word[idx] for idx in best_seq[1:]
                     if idx != vocab.word2idx[vocab.end_token]]
            return ' '.join(result)

        return ""

# gradio interface
demo = gr.Interface(
    fn=generate_caption,
    inputs=[
        gr.Image(label="Upload Image"),
        gr.Radio(["Greedy", "Beam Search"], value="Greedy", label="Search Method")
    ],
    outputs=gr.Textbox(label="Generated Caption"),
    title="Image Captioning with Seq2Seq",
    description="Upload an image to generate a caption using our trained model!"
)

if __name__ == "__main__":
    demo.launch()